# Compressing 5 TB of jsons into 5 GB through data normalization
It's simple: JSONs take up a lot of space. The nested format is very inefficient with redundant value entries. Any data engineer who has written custom code to clean up even the easiest files using progressive explode's and distinct's knows this is a cumbersome and ugly process. Shared here is a module based in [PySpark](https://spark.apache.org/docs/latest/api/python/index.html) that iteratively explodes and normalizes a df of structured data across multiple tables in Hive metastore (call [setCurrentCatalog](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Catalog.setCurrentCatalog.html) or [setCurrentDatabase](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Catalog.setCurrentDatabase.html) to use Unity Catalog instead) no matter the underlying schema.

Two functions are shared: the first normalizes a nested dataframe across multiple tables in the current catalog, and the second helps build sql queries against the returned dataset.

# Installation

The wheel dist\spark_json_table_normalization-0.0.1-py3-none-any.whl is already built and available. 

# proper_json_normalize

A dataframe and a prefix for the tables must be passed. All tables will start with "path_to_tables."  

Provide a "csv_path" if you wish for csvs to be generated in addition to tables: you will notice the text format takes significantly more space than the tables.

```
from proper_json_normalize import proper_json_normalize

data_prefix="all_tables_start_with_this_string"
t = time.perf_counter()
final_tables = proper_json_normalize(
  df=df, 
  path_to_tables=data_prefix, 
  csv_path=None
)
```

# generate_query

Normalizing the JSONs creates dozens of tables, and crafting sql against them is cumbersome. The second method shared returns a query that can be run via "spark.sql" to reconstruct any desired columns from the original dataset. Use a prefix and underscore to indicate the path to a given struct, and select "_distinct_id" if you wish to use the autogenerated identity columns.

```
from proper_json_normalize import generate_query
from pyspark.sql import functions as F

data_prefix="all_tables_start_with_this_string"
query = generate_query(
        data_prefix=data_prefix, 
        cols=[f"{data_prefix}_distinct_id", f"{data_prefix}_object_and_column_name"]
        )
print(query)
spark.sql(
    query
)
```